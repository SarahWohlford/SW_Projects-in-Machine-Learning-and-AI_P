{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SarahWohlford/SW_Projects-in-Machine-Learning-and-AI_P/blob/main/Proj_HW5_Q3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This is kinda messy im sorry\n",
        "\n",
        "It was hard\n",
        "\n",
        "Some code blocks are duplacits with some changes\n"
      ],
      "metadata": {
        "id": "kF547rDHuHZU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Task 3 (55 points): NLP and Attention Mechanism\n",
        "Part 1 (10 points): Implement the scaled dot-product attention as discussed in class\n",
        "(lecture 14) from scratch (use NumPy and pandas only, no deep learning libraries are\n",
        "allowed for this step)."
      ],
      "metadata": {
        "id": "V9rSaM_Lop84"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "def scaled_dot_product_attention(Q, K, V):\n",
        "\n",
        "    # Dot product between Q and K^T\n",
        "    matmul_qk = np.matmul(Q, K.T)  # shape (..., seq_len_q, seq_len_k)\n",
        "\n",
        "    #  Scale by sqrt(dk)\n",
        "    dk = K.shape[-1]\n",
        "    scaled_attention_logits = matmul_qk / np.sqrt(dk)\n",
        "\n",
        "    #  Apply softmax to get attention weights\n",
        "    attention_weights = np.exp(scaled_attention_logits)\n",
        "    attention_weights /= np.sum(attention_weights, axis=-1, keepdims=True)\n",
        "\n",
        "    # Multiply weights with V\n",
        "    output = np.matmul(attention_weights, V)\n",
        "\n",
        "    return output, attention_weights\n"
      ],
      "metadata": {
        "id": "_gOnMG-qLknU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Part 2 (10 points): Pick any encoder-decoder seq2seq model (as discussed in class) and\n",
        "integrate the scaled dot-product attention in the encoder architecture. You may come\n",
        "up with your own technique of integration or adopt one from literature. Hint: See\n",
        "Bahdanau or Luong attention paper presented in class (lecture 14)."
      ],
      "metadata": {
        "id": "lc_kp014pNVv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "\n",
        "def scaled_dot_product_attention(Q, K, V, mask=None):\n",
        "\n",
        "    matmul_qk = tf.matmul(Q, K, transpose_b=True)                 # (batch, 1, seq_len)\n",
        "    dk = tf.cast(tf.shape(K)[-1], tf.float32)\n",
        "    logits = matmul_qk / tf.math.sqrt(dk)                         # scale\n",
        "    if mask is not None:\n",
        "        logits += (mask * -1e9)\n",
        "    weights = tf.nn.softmax(logits, axis=-1)                      # (batch, 1, seq_len)\n",
        "    output = tf.matmul(weights, V)                                # (batch, 1, d_model)\n",
        "    return output, weights\n",
        "\n",
        "class Encoder(tf.keras.Model):\n",
        "    def __init__(self, vocab_size, embedding_dim, enc_units):\n",
        "        super().__init__()\n",
        "        self.enc_units = enc_units\n",
        "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
        "        self.lstm = tf.keras.layers.LSTM(\n",
        "            enc_units, return_sequences=True, return_state=True\n",
        "        )\n",
        "\n",
        "    def call(self, x, hidden_state):\n",
        "        x = self.embedding(x)                                       # (batch, seq_len, emb)\n",
        "        output, h, c = self.lstm(x, initial_state=hidden_state)\n",
        "        # output: (batch, seq_len, enc_units)\n",
        "        return output, (h, c)\n",
        "\n",
        "    def initialize_hidden_state(self, batch_size):\n",
        "        return (tf.zeros((batch_size, self.enc_units)),\n",
        "                tf.zeros((batch_size, self.enc_units)))\n",
        "\n",
        "\n",
        "class DecoderWithAttention(tf.keras.Model):\n",
        "    def __init__(self, vocab_size, embedding_dim, dec_units):\n",
        "        super().__init__()\n",
        "        self.dec_units = dec_units\n",
        "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
        "        self.lstm = tf.keras.layers.LSTM(\n",
        "            dec_units, return_sequences=True, return_state=True\n",
        "        )\n",
        "        # We’ll concat context + embedding before feeding to LSTM\n",
        "        self.fc = tf.keras.layers.Dense(vocab_size)\n",
        "\n",
        "    def call(self, x, hidden_state, enc_output, enc_mask=None):\n",
        "        \"\"\"\n",
        "        x: (batch, 1) — current input token\n",
        "        hidden_state: tuple (h, c) each (batch, dec_units)\n",
        "        enc_output: (batch, seq_len, dec_units)\n",
        "        enc_mask: (batch, 1, seq_len) or None\n",
        "        \"\"\"\n",
        "        #  embed the decoder input\n",
        "        x = self.embedding(x)                                       # (batch, 1, emb)\n",
        "        # compute attention context using last hidden state as query\n",
        "        h_prev = tf.expand_dims(hidden_state[0], 1)                 # (batch, 1, dec_units)\n",
        "        context, attn_weights = scaled_dot_product_attention(\n",
        "            h_prev, enc_output, enc_output, mask=enc_mask\n",
        "        )\n",
        "        # concat context + embed\n",
        "        x = tf.concat([context, x], axis=-1)                        # (batch, 1, emb+units)\n",
        "        # pass through LSTM\n",
        "        output, h, c = self.lstm(x, initial_state=hidden_state)\n",
        "        # project to vocab\n",
        "        logits = self.fc(output)                                    # (batch, 1, vocab_size)\n",
        "        return logits, (h, c), attn_weights\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "tUVUEXSlr8cG"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Part 4 (30 points): In this part you are required to implement a simplified Transformer\n",
        "model from scratch (using Python and NumPy/PyTorch/TensorFlow with minimal high level abstractions) and apply it to a machine translation task (e.g., English-to-French or\n",
        "English-to-German translation) using the same dataset from part 3"
      ],
      "metadata": {
        "id": "khBQ_vnvpfst"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f7fd25bb"
      },
      "source": [
        "\n",
        "Create a function or layer to generate positional encodings that will be added to the input embeddings to inject sequence order information.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4105d7a6"
      },
      "source": [
        "def positional_encoding(position, d_model):\n",
        "\n",
        "    angle_rads = np.arange(position)[:, np.newaxis] / np.power(10000, (2 * (np.arange(d_model)[np.newaxis, :] // 2)) / d_model)\n",
        "\n",
        "    # apply sin\n",
        "    angle_rads[:, 0::2] = np.sin(angle_rads[:, 0::2])\n",
        "\n",
        "    # apply cos\n",
        "    angle_rads[:, 1::2] = np.cos(angle_rads[:, 1::2])\n",
        "\n",
        "    pos_encoding = angle_rads[np.newaxis, ...]\n",
        "\n",
        "    return pos_encoding\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ff8a9579"
      },
      "source": [
        "Build a multi-head attention mechanism using the scaled dot-product attention function as a base. This will involve splitting the input into multiple heads, applying scaled dot-product attention to each head, and then concatenating the results.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "92071483"
      },
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "\n",
        "class MultiHeadAttention(tf.keras.layers.Layer):\n",
        "    def __init__(self, d_model, num_heads):\n",
        "        super(MultiHeadAttention, self).__init__()\n",
        "        self.num_heads = num_heads\n",
        "        self.d_model = d_model\n",
        "\n",
        "        assert d_model % self.num_heads == 0\n",
        "\n",
        "        self.depth = d_model // self.num_heads\n",
        "\n",
        "        self.wq = tf.keras.layers.Dense(d_model)\n",
        "        self.wk = tf.keras.layers.Dense(d_model)\n",
        "        self.wv = tf.keras.layers.Dense(d_model)\n",
        "\n",
        "        self.dense = tf.keras.layers.Dense(d_model)\n",
        "\n",
        "    def split_heads(self, x, batch_size):\n",
        "        #Split the last dimension into (num_heads, depth).\n",
        "        #Transpose the result such that the shape is (batch_size, num_heads, seq_len, depth)\n",
        "\n",
        "        x = tf.reshape(x, (batch_size, -1, self.num_heads, self.depth))\n",
        "        return tf.transpose(x, perm=[0, 2, 1, 3])\n",
        "\n",
        "    def combine_heads(self, x, batch_size):\n",
        "        #Combine the heads back to (batch_size, seq_len_q, d_model)\n",
        "        x = tf.transpose(x, perm=[0, 2, 1, 3])\n",
        "        return tf.reshape(x, (batch_size, -1, self.d_model))\n",
        "\n",
        "    def call(self, v, k, q):\n",
        "        batch_size = tf.shape(q)[0]\n",
        "\n",
        "        q = self.wq(q)  # (batch_size, seq_len, d_model)\n",
        "        k = self.wk(k)  # (batch_size, seq_len, d_model)\n",
        "        v = self.wv(v)  # (batch_size, seq_len, d_model)\n",
        "\n",
        "        q = self.split_heads(q, batch_size)  # (batch_size, num_heads, seq_len_q, depth)\n",
        "        k = self.split_heads(k, batch_size)  # (batch_size, num_heads, seq_len_k, depth)\n",
        "        v = self.split_heads(v, batch_size)  # (batch_size, num_heads, seq_len_v, depth)\n",
        "\n",
        "        scaled_attention, attention_weights = scaled_dot_product_attention(q, k, v)\n",
        "\n",
        "        # Combine the heads back\n",
        "        concat_attention = self.combine_heads(scaled_attention, batch_size)  # (batch_size, seq_len_q, d_model)\n",
        "\n",
        "        # Final linear layer\n",
        "        output = self.dense(concat_attention)  # (batch_size, seq_len_q, d_model)\n",
        "\n",
        "        return output, attention_weights\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ca96e88b"
      },
      "source": [
        "rewrite the scaled_dot_product_attention function using TensorFlow operations so it can be used within the TensorFlow MultiHeadAttention layer.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ac64363d"
      },
      "source": [
        "def scaled_dot_product_attention(q, k, v, mask=None):\n",
        "    \"\"\"\n",
        "    Compute scaled dot-product attention using TensorFlow operations.\n",
        "\n",
        "    Args:\n",
        "        q: Query tensor of shape (..., seq_len_q, depth)\n",
        "        k: Key tensor of shape (..., seq_len_k, depth)\n",
        "        v: Value tensor of shape (..., seq_len_v, depth_v)\n",
        "        mask: Optional mask tensor (broadcastable to (..., seq_len_q, seq_len_k)). Defaults to None.\n",
        "\n",
        "    Returns:\n",
        "        output: Attention output tensor (..., seq_len_q, depth_v)\n",
        "        attention_weights: Attention weights tensor (..., seq_len_q, seq_len_k)\n",
        "    \"\"\"\n",
        "    #Dot product between Q and K^T\n",
        "    matmul_qk = tf.matmul(q, k, transpose_b=True)  # (..., seq_len_q, seq_len_k)\n",
        "\n",
        "    #cale by sqrt(dk)\n",
        "    dk = tf.cast(tf.shape(k)[-1], tf.float32)\n",
        "    scaled_attention_logits = matmul_qk / tf.math.sqrt(dk)\n",
        "\n",
        "\n",
        "    if mask is not None:\n",
        "        scaled_attention_logits += (mask * -1e9) # Add a large negative number to masked positions\n",
        "\n",
        "    # Apply softmax to get attention weights\n",
        "    attention_weights = tf.nn.softmax(scaled_attention_logits, axis=-1)  # (..., seq_len_q, seq_len_k)\n",
        "\n",
        "    #Multiply weights with V\n",
        "    output = tf.matmul(attention_weights, v)  # (..., seq_len_q, depth_v)\n",
        "\n",
        "    return output, attention_weights\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "07c3e8a5"
      },
      "source": [
        "Single encoder layer that includes multi-head self-attention, a feed-forward network, residual connections, and layer normalization as described in the simplified architecture (2 layers total).\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2676186e"
      },
      "source": [
        "class EncoderLayer(tf.keras.layers.Layer):\n",
        "    def __init__(self, d_model, num_heads, ff_dim, rate=0.1):\n",
        "        super(EncoderLayer, self).__init__()\n",
        "\n",
        "        self.mha = MultiHeadAttention(d_model, num_heads)\n",
        "        self.ffn = tf.keras.Sequential([\n",
        "            tf.keras.layers.Dense(ff_dim, activation='relu'),\n",
        "            tf.keras.layers.Dense(d_model)\n",
        "        ])\n",
        "\n",
        "        self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "\n",
        "        self.dropout1 = tf.keras.layers.Dropout(rate)\n",
        "        self.dropout2 = tf.keras.layers.Dropout(rate)\n",
        "\n",
        "    def call(self, x, training, mask=None):\n",
        "        # Multi-head attention sublayer\n",
        "        attn_output, _ = self.mha(x, x, x, mask)  # (batch_size, input_seq_len, d_model)\n",
        "        attn_output = self.dropout1(attn_output, training=training)\n",
        "        out1 = self.layernorm1(x + attn_output)  # (batch_size, input_seq_len, d_model)\n",
        "\n",
        "        # Feed-forward network sublayer\n",
        "        ffn_output = self.ffn(out1)  # (batch_size, input_seq_len, d_model)\n",
        "        ffn_output = self.dropout2(ffn_output, training=training)\n",
        "        out2 = self.layernorm2(out1 + ffn_output)  # (batch_size, input_seq_len, d_model)\n",
        "\n",
        "        return out2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "977464d5"
      },
      "source": [
        "Build the full encoder by stacking the specified number of encoder layers (2 layers).\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d373f75a"
      },
      "source": [
        "class Encoder(tf.keras.layers.Layer):\n",
        "    def __init__(self, num_layers, d_model, num_heads, ff_dim, input_vocab_size, maximum_position_encoding, rate=0.1):\n",
        "        super(Encoder, self).__init__()\n",
        "\n",
        "        self.d_model = d_model\n",
        "        self.num_layers = num_layers\n",
        "\n",
        "        self.embedding = tf.keras.layers.Embedding(input_vocab_size, d_model)\n",
        "        self.pos_encoding = positional_encoding(maximum_position_encoding, self.d_model)\n",
        "\n",
        "        self.enc_layers = [EncoderLayer(d_model, num_heads, ff_dim, rate) for _ in range(num_layers)]\n",
        "\n",
        "        self.dropout = tf.keras.layers.Dropout(rate)\n",
        "\n",
        "    def call(self, x, training, mask=None):\n",
        "        seq_len = tf.shape(x)[1]\n",
        "\n",
        "        # Add embedding and positional encoding.\n",
        "        x = self.embedding(x)  # (batch_size, input_seq_len, d_model)\n",
        "        x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
        "        x += self.pos_encoding[:, :seq_len, :]\n",
        "\n",
        "        x = self.dropout(x, training=training)\n",
        "\n",
        "        for i in range(self.num_layers):\n",
        "            x = self.enc_layers[i](x, training, mask)\n",
        "\n",
        "        return x  # (batch_size, input_seq_len, d_model)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b5b00894"
      },
      "source": [
        "Decoder layer that includes masked multi-head self-attention (to prevent attending to future tokens), multi-head attention over the encoder's output, a feed-forward network, residual connections, and layer normalization (2 layers total).\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1eabaada"
      },
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "class DecoderLayer(tf.keras.layers.Layer):\n",
        "    def __init__(self, d_model, num_heads, ff_dim, rate=0.1):\n",
        "        super(DecoderLayer, self).__init__()\n",
        "\n",
        "        self.mha1 = MultiHeadAttention(d_model, num_heads)  # Masked self-attention\n",
        "        self.mha2 = MultiHeadAttention(d_model, num_heads)  # Encoder-decoder attention\n",
        "\n",
        "        self.ffn = tf.keras.Sequential([\n",
        "            tf.keras.layers.Dense(ff_dim, activation='relu'),\n",
        "            tf.keras.layers.Dense(d_model)\n",
        "        ])\n",
        "\n",
        "        self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.layernorm3 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "\n",
        "        self.dropout1 = tf.keras.layers.Dropout(rate)\n",
        "        self.dropout2 = tf.keras.layers.Dropout(rate)\n",
        "        self.dropout3 = tf.keras.layers.Dropout(rate)\n",
        "\n",
        "    def call(self, x, enc_output, training, look_ahead_mask, padding_mask):\n",
        "        # Masked multi-head self-attention\n",
        "        attn1, attn_weights_block1 = self.mha1(x, x, x, look_ahead_mask)\n",
        "        attn1 = self.dropout1(attn1, training=training)\n",
        "        out1 = self.layernorm1(x + attn1)\n",
        "\n",
        "        # Multi-head attention over encoder output\n",
        "        attn2, attn_weights_block2 = self.mha2(\n",
        "            enc_output, enc_output, out1, padding_mask)  # Use padding_mask here\n",
        "        attn2 = self.dropout2(attn2, training=training)\n",
        "        out2 = self.layernorm2(out1 + attn2)\n",
        "\n",
        "        # Feed-forward network\n",
        "        ffn_output = self.ffn(out2)\n",
        "        ffn_output = self.dropout3(ffn_output, training=training)\n",
        "        out3 = self.layernorm3(out2 + ffn_output)\n",
        "\n",
        "        return out3, attn_weights_block1, attn_weights_block2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "53948d12"
      },
      "source": [
        "Build the full decoder by stacking the specified number of decoder layers (2 layers).\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9a186a4f"
      },
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np # Keep numpy import for positional_encoding if it's not a tf.keras.layers.Layer\n",
        "\n",
        "class Decoder(tf.keras.layers.Layer):\n",
        "    def __init__(self, num_layers, d_model, num_heads, ff_dim, target_vocab_size, maximum_position_encoding, rate=0.1):\n",
        "        super(Decoder, self).__init__()\n",
        "\n",
        "        self.d_model = d_model\n",
        "        self.num_layers = num_layers\n",
        "\n",
        "        self.embedding = tf.keras.layers.Embedding(target_vocab_size, d_model)\n",
        "        self.pos_encoding = positional_encoding(maximum_position_encoding, d_model)\n",
        "\n",
        "        self.dec_layers = [DecoderLayer(d_model, num_heads, ff_dim, rate) for _ in range(num_layers)]\n",
        "\n",
        "        self.dropout = tf.keras.layers.Dropout(rate)\n",
        "\n",
        "    def call(self, x, enc_output, training, look_ahead_mask, padding_mask):\n",
        "        seq_len = tf.shape(x)[1]\n",
        "        attention_weights = {}\n",
        "\n",
        "        # Add embedding and positional encoding.\n",
        "        x = self.embedding(x)  # (batch_size, target_seq_len, d_model)\n",
        "        x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
        "        x += self.pos_encoding[:, :seq_len, :]\n",
        "\n",
        "        x = self.dropout(x, training=training)\n",
        "\n",
        "        for i in range(self.num_layers):\n",
        "            x, block1, block2 = self.dec_layers[i](x, enc_output, training, look_ahead_mask, padding_mask)\n",
        "\n",
        "            attention_weights[f'decoder_layer{i+1}_block1'] = block1\n",
        "            attention_weights[f'decoder_layer{i+1}_block2'] = block2\n",
        "\n",
        "        # x.shape == (batch_size, target_seq_len, d_model)\n",
        "        return x, attention_weights\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "55ea7290"
      },
      "source": [
        "Transformer model that combines the encoder and decoder.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "54cf0509"
      },
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "class Transformer(tf.keras.Model):\n",
        "    def __init__(self, num_layers, d_model, num_heads, ff_dim, input_vocab_size, target_vocab_size, maximum_position_encoding, rate=0.1):\n",
        "        super(Transformer, self).__init__()\n",
        "\n",
        "        self.encoder = Encoder(num_layers, d_model, num_heads, ff_dim,\n",
        "                               input_vocab_size, maximum_position_encoding, rate)\n",
        "\n",
        "        self.decoder = Decoder(num_layers, d_model, num_heads, ff_dim,\n",
        "                               target_vocab_size, maximum_position_encoding, rate)\n",
        "\n",
        "        self.final_layer = tf.keras.layers.Dense(target_vocab_size, activation='softmax')\n",
        "\n",
        "\n",
        "    def call(self, inp, tar, training, enc_padding_mask, look_ahead_mask, dec_padding_mask):\n",
        "\n",
        "        enc_output = self.encoder(inp, training, enc_padding_mask)  # (batch_size, inp_seq_len, d_model)\n",
        "\n",
        "        # dec_output.shape == (batch_size, tar_seq_len, d_model)\n",
        "        dec_output, attention_weights = self.decoder(\n",
        "            tar, enc_output, training, look_ahead_mask, dec_padding_mask)\n",
        "\n",
        "        final_output = self.final_layer(dec_output)  # (batch_size, tar_seq_len, target_vocab_size)\n",
        "\n",
        "        return final_output, attention_weights\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0c285931"
      },
      "source": [
        "Utilize the data loading and subword tokenization setup from Part 3 (using the ted_hrlr_translate/pt_to_en dataset ) to prepare the data for training.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b1e05974",
        "outputId": "73a05acb-4df2-4f09-f1c2-ae492121f018"
      },
      "source": [
        "import tensorflow_datasets as tfds\n",
        "import tensorflow as tf\n",
        "\n",
        "# Load the dataset\n",
        "examples, metadata = tfds.load('ted_hrlr_translate/pt_to_en', with_info=True, as_supervised=True)\n",
        "train_examples, val_examples = examples['train'], examples['validation']\n",
        "\n",
        "# Create subword tokenizers\n",
        "tokenizer_pt = tfds.deprecated.text.SubwordTextEncoder.build_from_corpus(\n",
        "    (pt.numpy() for pt, en in train_examples), target_vocab_size=2**13)\n",
        "\n",
        "tokenizer_en = tfds.deprecated.text.SubwordTextEncoder.build_from_corpus(\n",
        "    (en.numpy() for pt, en in train_examples), target_vocab_size=2**13)\n",
        "\n",
        "# Print vocabulary sizes\n",
        "print(f\"Portuguese vocabulary size: {tokenizer_pt.vocab_size}\")\n",
        "print(f\"English vocabulary size: {tokenizer_en.vocab_size}\")\n",
        "\n",
        "# Define maximum sequence length\n",
        "MAX_LENGTH = 40"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Portuguese vocabulary size: 8214\n",
            "English vocabulary size: 8087\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2233fe20"
      },
      "source": [
        "Apply the encoding and padding function to the dataset, then filter, cache, shuffle, and batch the data.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 219
        },
        "id": "8f238848",
        "outputId": "293309eb-38cd-4a89-d196-d78036604470"
      },
      "source": [
        "def encode_and_pad(pt, en):\n",
        "    pt_tokens = [tokenizer_pt.vocab_size] + tokenizer_pt.encode(pt.numpy()) + [tokenizer_pt.vocab_size + 1]\n",
        "    en_tokens = [tokenizer_en.vocab_size] + tokenizer_en.encode(en.numpy()) + [tokenizer_en.vocab_size + 1]\n",
        "\n",
        "    # Pad and truncate\n",
        "    pt_padded = pt_tokens[:MAX_LENGTH] + [0] * max(0, MAX_LENGTH - len(pt_tokens))\n",
        "    en_padded = en_tokens[:MAX_LENGTH] + [0] * max(0, MAX_LENGTH - len(en_tokens))\n",
        "\n",
        "    return tf.constant(pt_padded, dtype=tf.int64), tf.constant(en_padded, dtype=tf.int64)\n",
        "\n",
        "# Apply the encoding and padding function\n",
        "train_dataset = train_examples.map(lambda pt, en: tf.py_function(func=encode_and_pad, inp=[pt, en], Tout=[tf.int64, tf.int66]))\n",
        "val_dataset = val_examples.map(lambda pt, en: tf.py_function(func=encode_and_pad, inp=[pt, en], Tout=[tf.int64, tf.int66]))\n",
        "\n",
        "# Filter out empty sequences (which should not happen with start/end tokens, but as a safeguard)\n",
        "train_dataset = train_dataset.filter(lambda pt, en: tf.logical_and(tf.size(pt) > 0, tf.size(en) > 0))\n",
        "val_dataset = val_dataset.filter(lambda pt, en: tf.logical_and(tf.size(pt) > 0, tf.size(en) > 0))\n",
        "\n",
        "\n",
        "# Cache, shuffle, and batch\n",
        "BUFFER_SIZE = 20000\n",
        "BATCH_SIZE = 64\n",
        "\n",
        "train_dataset = train_dataset.cache().shuffle(BUFFER_SIZE).batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)\n",
        "val_dataset = val_dataset.cache().batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)\n"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'train_examples' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1394380821.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;31m# Apply the encoding and padding function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0mtrain_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_examples\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mpt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0men\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpy_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mencode_and_pad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0men\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mint64\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mint66\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0mval_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mval_examples\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mpt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0men\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpy_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mencode_and_pad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0men\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mint64\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mint66\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'train_examples' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "I cant fix :/ IDK"
      ],
      "metadata": {
        "id": "UfwNBTHIs6oh"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kvbx4X-iZ0r3"
      },
      "source": [
        "def encode_and_pad(pt, en):\n",
        "    pt_tokens = [tokenizer_pt.vocab_size] + tokenizer_pt.encode(pt.numpy()) + [tokenizer_pt.vocab_size + 1]\n",
        "    en_tokens = [tokenizer_en.vocab_size] + tokenizer_en.encode(en.numpy()) + [tokenizer_en.vocab_size + 1]\n",
        "\n",
        "    # Pad and truncate\n",
        "    pt_padded = pt_tokens[:MAX_LENGTH] + [0] * max(0, MAX_LENGTH - len(pt_tokens))\n",
        "    en_padded = en_tokens[:MAX_LENGTH] + [0] * max(0, MAX_LENGTH - len(en_tokens))\n",
        "\n",
        "    return tf.constant(pt_padded, dtype=tf.int64), tf.constant(en_padded, dtype=tf.int64)\n",
        "\n",
        "# Apply the encoding and padding function\n",
        "train_dataset = train_examples.map(lambda pt, en: tf.py_function(func=encode_and_pad, inp=[pt, en], Tout=[tf.int64, tf.int64]))\n",
        "val_dataset = val_examples.map(lambda pt, en: tf.py_function(func=encode_and_pad, inp=[pt, en], Tout=[tf.int64, tf.int64]))\n",
        "\n",
        "# Filter out empty sequences (which should not happen with start/end tokens, but as a safeguard)\n",
        "train_dataset = train_dataset.filter(lambda pt, en: tf.logical_and(tf.size(pt) > 0, tf.size(en) > 0))\n",
        "val_dataset = val_dataset.filter(lambda pt, en: tf.logical_and(tf.size(pt) > 0, tf.size(en) > 0))\n",
        "\n",
        "\n",
        "# Cache, shuffle, and batch\n",
        "BUFFER_SIZE = 20000\n",
        "BATCH_SIZE = 64\n",
        "\n",
        "train_dataset = train_dataset.cache().shuffle(BUFFER_SIZE).batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)\n",
        "val_dataset = val_dataset.cache().batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2b82d832"
      },
      "source": [
        "training setup defined the loss function, optimizer, and metrics for training.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "85cde308"
      },
      "source": [
        "class CustomSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
        "    def __init__(self, d_model, warmup_steps=4000):\n",
        "        super(CustomSchedule, self).__init__()\n",
        "\n",
        "        self.d_model = tf.cast(d_model, tf.float32)\n",
        "        self.warmup_steps = warmup_steps\n",
        "\n",
        "    def __call__(self, step):\n",
        "        arg1 = tf.math.rsqrt(step)\n",
        "        arg2 = step * (self.warmup_steps ** -1.5)\n",
        "\n",
        "        return tf.math.rsqrt(self.d_model) * tf.math.minimum(arg1, arg2)\n",
        "\n",
        "# Define the loss function\n",
        "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False, reduction='none')\n",
        "\n",
        "def loss_function(real, pred):\n",
        "    mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
        "    loss_ = loss_object(real, pred)\n",
        "\n",
        "    mask = tf.cast(mask, dtype=loss_.dtype)\n",
        "    loss_ *= mask\n",
        "\n",
        "    return tf.reduce_sum(loss_)/tf.reduce_sum(mask)\n",
        "\n",
        "# Define the optimizer\n",
        "learning_rate = CustomSchedule(d_model)\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate, beta_1=0.9, beta_2=0.98, epsilon=1e-9)\n",
        "\n",
        "# Define training metrics\n",
        "train_loss = tf.keras.metrics.Mean(name='train_loss')\n",
        "train_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name='train_accuracy')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3ebca4cc"
      },
      "source": [
        "Training loop\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "55523575"
      },
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import time\n",
        "import tensorflow_datasets as tfds\n",
        "\n",
        "# Assume necessary components (Transformer, tokenizer_pt, tokenizer_en, MAX_LENGTH, d_model) are defined\n",
        "\n",
        "def translate_sentence(sentence, transformer, tokenizer_pt, tokenizer_en, MAX_LENGTH, d_model):\n",
        "    \"\"\"\n",
        "    Translates a sentence using the trained Transformer model.\n",
        "        sentence: The source sentence string (Portuguese).\n",
        "        transformer: The trained Transformer model.\n",
        "        tokenizer_pt: The source language (Portuguese) tokenizer.\n",
        "        tokenizer_en: The target language (English) tokenizer.\n",
        "        MAX_LENGTH: Maximum sequence length.\n",
        "        d_model: Model dimensionality.\n",
        "\n",
        "    Returns: translated_sentence: The translated sentence string (English).\n",
        "    \"\"\"\n",
        "    # Encode the source sentence and prepare input\n",
        "    sentence = tf.constant(sentence, dtype=tf.string)\n",
        "    sentence = sentence.numpy().decode('utf-8')\n",
        "    sentence = [tokenizer_pt.vocab_size] + tokenizer_pt.encode(sentence) + [tokenizer_pt.vocab_size + 1]\n",
        "    sentence = tf.keras.preprocessing.sequence.pad_sequences([sentence],\n",
        "                                                             maxlen=MAX_LENGTH,\n",
        "                                                             padding='post')\n",
        "    encoder_input = tf.cast(sentence, dtype=tf.int64)\n",
        "\n",
        "    # Create encoder padding mask\n",
        "    enc_padding_mask = create_padding_mask(encoder_input)\n",
        "\n",
        "    # 2. Obtain encoder output (evaluation mode)\n",
        "    encoder_output = transformer.encoder(encoder_input, training=False, mask=enc_padding_mask)\n",
        "    # encoder_output.shape == (1, MAX_LENGTH, d_model)\n",
        "\n",
        "    # Initialize decoder input with the start token\n",
        "\n",
        "    start_token = tokenizer_en.vocab_size\n",
        "    end_token = tokenizer_en.vocab_size + 1\n",
        "    decoder_input = tf.fill([1, MAX_LENGTH], 0, dtype=tf.int64)\n",
        "    decoder_input = tf.tensor_scatter_nd_update(decoder_input, [[0, 0]], [start_token]) # Shape: (1, MAX_LENGTH)\n",
        "\n",
        "    output_sequence = tf.TensorArray(dtype=tf.int64, size=MAX_LENGTH) # Store generated tokens\n",
        "\n",
        "    # 4. Iterative decoding loop\n",
        "    for i in range(MAX_LENGTH):\n",
        "        current_decoder_input = decoder_input[:, :i+1] # Take up to current position\n",
        "        # current_decoder_input.shape == (1, i+1)\n",
        "\n",
        "\n",
        "        look_ahead_mask = create_look_ahead_mask(tf.shape(current_decoder_input)[1])\n",
        "\n",
        "        dec_target_padding_mask = create_padding_mask(current_decoder_input)\n",
        "        combined_mask = tf.maximum(dec_target_padding_mask, look_ahead_mask)\n",
        "\n",
        "\n",
        "        predictions, attention_weights = transformer.decoder(\n",
        "            current_decoder_input,\n",
        "            encoder_output,\n",
        "            training=False,\n",
        "            look_ahead_mask=combined_mask,\n",
        "            padding_mask=enc_padding_mask # Use encoder padding mask for cross-attention\n",
        "        )\n",
        "\n",
        "\n",
        "        # Pass decoder output through the final layer\n",
        "        predictions = transformer.final_layer(predictions) # (1, i+1, target_vocab_size)\n",
        "\n",
        "\n",
        "        predicted_token_id = tf.argmax(predictions[:, -1, :], axis=-1)[0] # Shape: scalar tf.int64\n",
        "\n",
        "        # Store the predicted token\n",
        "        output_sequence = output_sequence.write(i, predicted_token_id)\n",
        "\n",
        "        # Check if the predicted token is the end token\n",
        "        if predicted_token_id == end_token:\n",
        "            break\n",
        "\n",
        "\n",
        "        decoder_input = tf.tensor_scatter_nd_update(decoder_input, [[0, i + 1]], [predicted_token_id])\n",
        "\n",
        "\n",
        "    # Decode the generated sequence of token IDs\n",
        "    output_sequence = output_sequence.stack() # Shape: (generated_seq_len,)\n",
        "\n",
        "\n",
        "    generated_tokens = output_sequence.numpy() # Convert to numpy array\n",
        "\n",
        "    # Decode tokens back to string\n",
        "    translated_sentence = tokenizer_en.decode([token for token in generated_tokens if token < tokenizer_en.vocab_size])\n",
        "\n",
        "    return translated_sentence"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b8e226ec"
      },
      "source": [
        "BLEU score on the test set using the inference function.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c186bc00"
      },
      "source": [
        "Train the implemented Transformer model using the prepared dataset and training setup.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gsTqS9Chcnhw",
        "outputId": "d0e805b0-d682-4a17-d131-6cd930d7cb21"
      },
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np # Keep numpy for positional_encoding function\n",
        "import time # Keep time for timing\n",
        "\n",
        "# Re-define the loss function and optimizer to ensure they are correctly initialized\n",
        "class CustomSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
        "    def __init__(self, d_model, warmup_steps=4000):\n",
        "        super(CustomSchedule, self).__init__()\n",
        "        self.d_model = tf.cast(d_model, tf.float32)\n",
        "        self.warmup_steps = warmup_steps\n",
        "\n",
        "    def __call__(self, step):\n",
        "        # Explicitly cast step to float32\n",
        "        step = tf.cast(step, dtype=tf.float32)\n",
        "        arg1 = tf.math.rsqrt(step)\n",
        "        arg2 = step * (self.warmup_steps ** -1.5)\n",
        "        return tf.math.rsqrt(self.d_model) * tf.math.minimum(arg1, arg2)\n",
        "\n",
        "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True, reduction='none')\n",
        "\n",
        "def loss_function(real, pred):\n",
        "    mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
        "    loss_ = loss_object(real, pred)\n",
        "    mask = tf.cast(mask, dtype=loss_.dtype)\n",
        "    loss_ *= mask\n",
        "    return tf.reduce_sum(loss_)/tf.reduce_sum(mask)\n",
        "\n",
        "\n",
        "\n",
        "learning_rate = CustomSchedule(d_model)\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate, beta_1=0.9, beta_2=0.98, epsilon=1e-9)\n",
        "\n",
        "\n",
        "train_loss = tf.keras.metrics.Mean(name='train_loss')\n",
        "train_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name='train_accuracy')\n",
        "\n",
        "\n",
        "# Define the training step with correct keyword arguments\n",
        "@tf.function\n",
        "def train_step(inp, tar):\n",
        "    tar_inp = tar[:, :-1]\n",
        "    tar_real = tar[:, 1:]\n",
        "\n",
        "    # Create masks\n",
        "    enc_padding_mask, combined_mask, dec_padding_mask = create_masks(inp, tar_inp)\n",
        "\n",
        "    with tf.GradientTape() as tape:\n",
        "        predictions, _ = transformer(inp=inp, tar=tar_inp,\n",
        "                                     training=True,\n",
        "                                     enc_padding_mask=enc_padding_mask,\n",
        "                                     look_ahead_mask=combined_mask,\n",
        "                                     dec_padding_mask=dec_padding_mask);\n",
        "        loss = loss_function(tar_real, predictions)\n",
        "\n",
        "    gradients = tape.gradient(loss, transformer.trainable_variables)\n",
        "    optimizer.apply_gradients(zip(gradients, transformer.trainable_variables))\n",
        "\n",
        "    train_loss.update_state(loss)\n",
        "\n",
        "    train_accuracy.update_state(tar_real, predictions)\n",
        "\n",
        "\n",
        "# Main training loop\n",
        "EPOCHS = 5\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "    start = time.time()\n",
        "\n",
        "    train_loss.reset_state()\n",
        "    train_accuracy.reset_state()\n",
        "\n",
        "    for (batch, (inp, tar)) in enumerate(train_dataset):\n",
        "        train_step(inp, tar)\n",
        "\n",
        "        if batch % 50 == 0:\n",
        "            print(f'Epoch {epoch + 1} Batch {batch} Loss {train_loss.result():.4f} Accuracy {train_accuracy.result():.4f}')\n",
        "\n",
        "    print(f'Epoch {epoch + 1} Loss {train_loss.result():.4f} Accuracy {train_accuracy.result():.4f}')\n",
        "    print(f'Time taken for 1 epoch: {time.time() - start:.2f} secs\\n')\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1 Batch 0 Loss 9.0133 Accuracy 0.0000\n",
            "Epoch 1 Batch 50 Loss 8.9716 Accuracy 0.0003\n",
            "Epoch 1 Batch 100 Loss 8.8838 Accuracy 0.0092\n",
            "Epoch 1 Batch 150 Loss 8.7818 Accuracy 0.0167\n",
            "Epoch 1 Batch 200 Loss 8.6548 Accuracy 0.0208\n",
            "Epoch 1 Batch 250 Loss 8.5033 Accuracy 0.0232\n",
            "Epoch 1 Batch 300 Loss 8.3305 Accuracy 0.0266\n",
            "Epoch 1 Batch 350 Loss 8.1482 Accuracy 0.0304\n",
            "Epoch 1 Batch 400 Loss 7.9743 Accuracy 0.0339\n",
            "Epoch 1 Batch 450 Loss 7.8156 Accuracy 0.0372\n",
            "Epoch 1 Batch 500 Loss 7.6721 Accuracy 0.0412\n",
            "Epoch 1 Batch 550 Loss 7.5365 Accuracy 0.0454\n",
            "Epoch 1 Batch 600 Loss 7.4082 Accuracy 0.0494\n",
            "Epoch 1 Batch 650 Loss 7.2864 Accuracy 0.0534\n",
            "Epoch 1 Batch 700 Loss 7.1683 Accuracy 0.0572\n",
            "Epoch 1 Batch 750 Loss 7.0569 Accuracy 0.0611\n",
            "Epoch 1 Batch 800 Loss 6.9514 Accuracy 0.0648\n",
            "Epoch 1 Loss 6.9338 Accuracy 0.0654\n",
            "Time taken for 1 epoch: 91.64 secs\n",
            "\n",
            "Epoch 2 Batch 0 Loss 5.2590 Accuracy 0.1262\n",
            "Epoch 2 Batch 50 Loss 5.2354 Accuracy 0.1275\n",
            "Epoch 2 Batch 100 Loss 5.1964 Accuracy 0.1287\n",
            "Epoch 2 Batch 150 Loss 5.1667 Accuracy 0.1309\n",
            "Epoch 2 Batch 200 Loss 5.1342 Accuracy 0.1331\n",
            "Epoch 2 Batch 250 Loss 5.0992 Accuracy 0.1351\n",
            "Epoch 2 Batch 300 Loss 5.0659 Accuracy 0.1372\n",
            "Epoch 2 Batch 350 Loss 5.0331 Accuracy 0.1388\n",
            "Epoch 2 Batch 400 Loss 5.0024 Accuracy 0.1409\n",
            "Epoch 2 Batch 450 Loss 4.9714 Accuracy 0.1428\n",
            "Epoch 2 Batch 500 Loss 4.9417 Accuracy 0.1447\n",
            "Epoch 2 Batch 550 Loss 4.9097 Accuracy 0.1465\n",
            "Epoch 2 Batch 600 Loss 4.8810 Accuracy 0.1485\n",
            "Epoch 2 Batch 650 Loss 4.8511 Accuracy 0.1504\n",
            "Epoch 2 Batch 700 Loss 4.8236 Accuracy 0.1523\n",
            "Epoch 2 Batch 750 Loss 4.7946 Accuracy 0.1542\n",
            "Epoch 2 Batch 800 Loss 4.7679 Accuracy 0.1558\n",
            "Epoch 2 Loss 4.7623 Accuracy 0.1560\n",
            "Time taken for 1 epoch: 32.93 secs\n",
            "\n",
            "Epoch 3 Batch 0 Loss 4.2073 Accuracy 0.1747\n",
            "Epoch 3 Batch 50 Loss 4.2416 Accuracy 0.1880\n",
            "Epoch 3 Batch 100 Loss 4.2291 Accuracy 0.1883\n",
            "Epoch 3 Batch 150 Loss 4.2101 Accuracy 0.1896\n",
            "Epoch 3 Batch 200 Loss 4.1951 Accuracy 0.1907\n",
            "Epoch 3 Batch 250 Loss 4.1818 Accuracy 0.1912\n",
            "Epoch 3 Batch 300 Loss 4.1667 Accuracy 0.1922\n",
            "Epoch 3 Batch 350 Loss 4.1451 Accuracy 0.1933\n",
            "Epoch 3 Batch 400 Loss 4.1233 Accuracy 0.1942\n",
            "Epoch 3 Batch 450 Loss 4.1032 Accuracy 0.1955\n",
            "Epoch 3 Batch 500 Loss 4.0804 Accuracy 0.1966\n",
            "Epoch 3 Batch 550 Loss 4.0617 Accuracy 0.1979\n",
            "Epoch 3 Batch 600 Loss 4.0439 Accuracy 0.1991\n",
            "Epoch 3 Batch 650 Loss 4.0244 Accuracy 0.2000\n",
            "Epoch 3 Batch 700 Loss 4.0069 Accuracy 0.2011\n",
            "Epoch 3 Batch 750 Loss 3.9891 Accuracy 0.2021\n",
            "Epoch 3 Batch 800 Loss 3.9692 Accuracy 0.2032\n",
            "Epoch 3 Loss 3.9662 Accuracy 0.2033\n",
            "Time taken for 1 epoch: 32.83 secs\n",
            "\n",
            "Epoch 4 Batch 0 Loss 3.6960 Accuracy 0.2280\n",
            "Epoch 4 Batch 50 Loss 3.5867 Accuracy 0.2249\n",
            "Epoch 4 Batch 100 Loss 3.5719 Accuracy 0.2246\n",
            "Epoch 4 Batch 150 Loss 3.5568 Accuracy 0.2255\n",
            "Epoch 4 Batch 200 Loss 3.5372 Accuracy 0.2261\n",
            "Epoch 4 Batch 250 Loss 3.5226 Accuracy 0.2272\n",
            "Epoch 4 Batch 300 Loss 3.5061 Accuracy 0.2283\n",
            "Epoch 4 Batch 350 Loss 3.4955 Accuracy 0.2286\n",
            "Epoch 4 Batch 400 Loss 3.4781 Accuracy 0.2294\n",
            "Epoch 4 Batch 450 Loss 3.4632 Accuracy 0.2305\n",
            "Epoch 4 Batch 500 Loss 3.4478 Accuracy 0.2312\n",
            "Epoch 4 Batch 550 Loss 3.4351 Accuracy 0.2321\n",
            "Epoch 4 Batch 600 Loss 3.4220 Accuracy 0.2326\n",
            "Epoch 4 Batch 650 Loss 3.4089 Accuracy 0.2334\n",
            "Epoch 4 Batch 700 Loss 3.3951 Accuracy 0.2341\n",
            "Epoch 4 Batch 750 Loss 3.3825 Accuracy 0.2351\n",
            "Epoch 4 Batch 800 Loss 3.3706 Accuracy 0.2359\n",
            "Epoch 4 Loss 3.3676 Accuracy 0.2360\n",
            "Time taken for 1 epoch: 33.01 secs\n",
            "\n",
            "Epoch 5 Batch 0 Loss 2.9639 Accuracy 0.2408\n",
            "Epoch 5 Batch 50 Loss 3.0136 Accuracy 0.2542\n",
            "Epoch 5 Batch 100 Loss 2.9885 Accuracy 0.2551\n",
            "Epoch 5 Batch 150 Loss 2.9711 Accuracy 0.2567\n",
            "Epoch 5 Batch 200 Loss 2.9593 Accuracy 0.2570\n",
            "Epoch 5 Batch 250 Loss 2.9505 Accuracy 0.2576\n",
            "Epoch 5 Batch 300 Loss 2.9427 Accuracy 0.2583\n",
            "Epoch 5 Batch 350 Loss 2.9330 Accuracy 0.2588\n",
            "Epoch 5 Batch 400 Loss 2.9245 Accuracy 0.2593\n",
            "Epoch 5 Batch 450 Loss 2.9171 Accuracy 0.2591\n",
            "Epoch 5 Batch 500 Loss 2.9080 Accuracy 0.2599\n",
            "Epoch 5 Batch 550 Loss 2.9014 Accuracy 0.2601\n",
            "Epoch 5 Batch 600 Loss 2.8932 Accuracy 0.2606\n",
            "Epoch 5 Batch 650 Loss 2.8877 Accuracy 0.2612\n",
            "Epoch 5 Batch 700 Loss 2.8815 Accuracy 0.2615\n",
            "Epoch 5 Batch 750 Loss 2.8726 Accuracy 0.2619\n",
            "Epoch 5 Batch 800 Loss 2.8643 Accuracy 0.2625\n",
            "Epoch 5 Loss 2.8638 Accuracy 0.2625\n",
            "Time taken for 1 epoch: 33.09 secs\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ae2a0d5f"
      },
      "source": [
        "Implement the calculation of the BLEU score on the test set using the inference function.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0d4d761f",
        "outputId": "5b9ac266-f8f8-43cf-9e27-3126c37100e7"
      },
      "source": [
        "import nltk\n",
        "import time\n",
        "\n",
        "references = []\n",
        "hypotheses = []\n",
        "\n",
        "\n",
        "if 'test_examples' not in locals():\n",
        "    print(\"Test dataset ('test_examples') not found. Using validation dataset ('val_examples') for BLEU evaluation.\")\n",
        "    test_examples = val_examples # Use validation data if test data is not loaded\n",
        "\n",
        "\n",
        "num_test_examples = 100 # Evaluate on 100 examples\n",
        "\n",
        "print(f\"Evaluating BLEU score on {num_test_examples} examples...\")\n",
        "start_time = time.time()\n",
        "\n",
        "\n",
        "\n",
        "for i, (pt_tensor, en_tensor) in enumerate(test_examples.take(num_test_examples)):\n",
        "    try:\n",
        "        # Get the translated sentence\n",
        "        translated_sentence = translate_sentence(pt_tensor, transformer, tokenizer_pt, tokenizer_en, MAX_LENGTH, d_model)\n",
        "\n",
        "        en_string = en_tensor.numpy().decode('utf-8')\n",
        "\n",
        "        reference_words = en_string.split()\n",
        "        hypothesis_words = translated_sentence.split()\n",
        "\n",
        "        references.append([reference_words]) # List of lists for references\n",
        "        hypotheses.append(hypothesis_words) # List of words for hypothesis\n",
        "\n",
        "        if (i + 1) % 20 == 0:\n",
        "            print(f\"Processed {i + 1}/{num_test_examples} examples.\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing example {i + 1}: {e}\")\n",
        "\n",
        "        continue\n",
        "\n",
        "\n",
        "try:\n",
        "    bleu_score = nltk.translate.bleu_score.corpus_bleu(\n",
        "        references,\n",
        "        hypotheses,\n",
        "        smoothing_function=nltk.translate.bleu_score.SmoothingFunction().method1\n",
        "    )\n",
        "    print(f\"\\nCorpus BLEU Score on {len(hypotheses)} examples: {bleu_score:.4f}\")\n",
        "except ZeroDivisionError:\n",
        "    print(\"\\nCould not compute BLEU score. This might happen if the generated hypotheses are empty.\")\n",
        "    bleu_score = 0.0\n",
        "    print(f\"Corpus BLEU Score: {bleu_score:.4f}\")\n",
        "\n",
        "end_time = time.time()\n",
        "print(f\"BLEU evaluation completed in {end_time - start_time:.2f} seconds.\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Evaluating BLEU score on 100 examples...\n",
            "Processed 20/100 examples.\n",
            "Error processing example 39: {{function_node __wrapped__TensorScatterUpdate_device_/job:localhost/replica:0/task:0/device:GPU:0}} indices[0] = [0, 40] does not index into shape [1,40] [Op:TensorScatterUpdate]\n",
            "Processed 40/100 examples.\n",
            "Processed 60/100 examples.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR:tensorflow:==================================\n",
            "Object was never used (type <class 'tensorflow.python.ops.tensor_array_ops.TensorArray'>):\n",
            "<tensorflow.python.ops.tensor_array_ops.TensorArray object at 0x7e93b070ce10>\n",
            "If you want to mark it as used call its \"mark_used()\" method.\n",
            "It was originally created here:\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\", line 3473, in run_ast_nodes\n",
            "    if (await self.run_code(code, result,  async_=asy)):  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\", line 3553, in run_code\n",
            "    exec(code_obj, self.user_global_ns, self.user_ns)  File \"/tmp/ipython-input-2517598569.py\", line 32, in <cell line: 0>\n",
            "    translated_sentence = translate_sentence(pt_tensor, transformer, tokenizer_pt, tokenizer_en, MAX_LENGTH, d_model)  File \"/tmp/ipython-input-1264527923.py\", line 75, in translate_sentence\n",
            "    decoder_input = tf.tensor_scatter_nd_update(decoder_input, [[0, i + 1]], [predicted_token_id])  File \"/usr/local/lib/python3.11/dist-packages/tensorflow/python/util/tf_should_use.py\", line 288, in wrapped\n",
            "    return _add_should_use_warning(fn(*args, **kwargs),\n",
            "==================================\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processed 80/100 examples.\n",
            "Error processing example 96: {{function_node __wrapped__TensorScatterUpdate_device_/job:localhost/replica:0/task:0/device:GPU:0}} indices[0] = [0, 40] does not index into shape [1,40] [Op:TensorScatterUpdate]\n",
            "Processed 100/100 examples.\n",
            "\n",
            "Corpus BLEU Score on 98 examples: 0.2033\n",
            "BLEU evaluation completed in 108.22 seconds.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qhlh1-YseDVR",
        "outputId": "3ab0c529-b388-4ade-90c9-9a686f8c66cb"
      },
      "source": [
        "import nltk\n",
        "import time\n",
        "\n",
        "\n",
        "references = []\n",
        "hypotheses = []\n",
        "\n",
        "if 'test_examples' not in locals():\n",
        "    print(\"Test dataset ('test_examples') not found. Using validation dataset ('val_examples') for BLEU evaluation.\")\n",
        "    test_examples = val_examples # Use validation data if test data is not loaded\n",
        "\n",
        "\n",
        "num_test_examples = 100 # Evaluate on 100 examples\n",
        "\n",
        "print(f\"Evaluating BLEU score on {num_test_examples} examples...\")\n",
        "start_time = time.time()\n",
        "\n",
        "\n",
        "\n",
        "# Re-define the translate_sentence function to handle the MAX_LENGTH update\n",
        "def translate_sentence(sentence, transformer, tokenizer_pt, tokenizer_en, MAX_LENGTH, d_model):\n",
        "    \"\"\"\n",
        "    Translates a sentence using the trained Transformer model.\n",
        "\n",
        "    Args:\n",
        "        sentence: The source sentence tensor (Portuguese).\n",
        "        transformer: The trained Transformer model.\n",
        "        tokenizer_pt: The source language (Portuguese) tokenizer.\n",
        "        tokenizer_en: The target language (English) tokenizer.\n",
        "        MAX_LENGTH: Maximum sequence length.\n",
        "        d_model: Model dimensionality.\n",
        "\n",
        "    Returns:\n",
        "        translated_sentence: The translated sentence string (English).\n",
        "    \"\"\"\n",
        "\n",
        "    sentence = sentence.numpy().decode('utf-8')\n",
        "    sentence = [tokenizer_pt.vocab_size] + tokenizer_pt.encode(sentence) + [tokenizer_pt.vocab_size + 1]\n",
        "    sentence = tf.keras.preprocessing.sequence.pad_sequences([sentence],\n",
        "                                                             maxlen=MAX_LENGTH,\n",
        "                                                             padding='post')\n",
        "    encoder_input = tf.cast(sentence, dtype=tf.int64) # Shape: (1, MAX_LENGTH)\n",
        "\n",
        "    # Create encoder padding mask\n",
        "    enc_padding_mask = create_padding_mask(encoder_input)\n",
        "\n",
        "    # Obtain encoder output\n",
        "    encoder_output = transformer.encoder(encoder_input, training=False)\n",
        "\n",
        "    # Initialize decoder input with the start token\n",
        "    start_token = tokenizer_en.vocab_size\n",
        "    end_token = tokenizer_en.vocab_size + 1\n",
        "    decoder_input = tf.fill([1, MAX_LENGTH], tf.constant(0, dtype=tf.int64)) # Pass value and dtype together\n",
        "    decoder_input = tf.tensor_scatter_nd_update(decoder_input, [[0, 0]], [start_token])\n",
        "\n",
        "    output_sequence = tf.TensorArray(dtype=tf.int64, size=MAX_LENGTH)\n",
        "\n",
        "    # decoding loop\n",
        "    for i in range(MAX_LENGTH):\n",
        "        current_decoder_input = decoder_input[:, :i+1]\n",
        "\n",
        "        look_ahead_mask = create_look_ahead_mask(tf.shape(current_decoder_input)[1])\n",
        "        dec_target_padding_mask = create_padding_mask(current_decoder_input)\n",
        "        combined_mask = tf.maximum(dec_target_padding_mask, look_ahead_mask)\n",
        "        decoder_cross_attention_padding_mask = enc_padding_mask\n",
        "\n",
        "        predictions, attention_weights = transformer.decoder(\n",
        "            current_decoder_input,\n",
        "            encoder_output,\n",
        "            training=False,\n",
        "            look_ahead_mask=combined_mask, # Mask for self-attention in decoder\n",
        "            padding_mask=decoder_cross_attention_padding_mask # Mask for cross-attention in decoder\n",
        "        )\n",
        "\n",
        "        predictions = transformer.final_layer(predictions)\n",
        "\n",
        "        predicted_token_id = tf.argmax(predictions[:, -1, :], axis=-1)[0]\n",
        "\n",
        "        if i < MAX_LENGTH - 1:\n",
        "             output_sequence = output_sequence.write(i, predicted_token_id)\n",
        "        else:\n",
        "             output_sequence = output_sequence.write(i, predicted_token_id)\n",
        "\n",
        "\n",
        "        if tf.equal(predicted_token_id, end_token):\n",
        "            break\n",
        "\n",
        "        # Only update decoder_input if not at the last index of MAX_LENGTH\n",
        "        if i < MAX_LENGTH - 1:\n",
        "             decoder_input = tf.tensor_scatter_nd_update(decoder_input, [[0, i + 1]], [predicted_token_id])\n",
        "\n",
        "\n",
        "    output_sequence = output_sequence.stack()\n",
        "\n",
        "\n",
        "    generated_tokens = output_sequence.numpy()\n",
        "    # Find the index of the first padding token (0) or end token\n",
        "    try:\n",
        "        first_padding_or_end_index = np.where((generated_tokens == 0) | (generated_tokens == end_token))[0][0]\n",
        "        generated_tokens = generated_tokens[:first_padding_or_end_index + 1]\n",
        "    except IndexError:\n",
        "\n",
        "        pass\n",
        "\n",
        "\n",
        "\n",
        "    translated_sentence = tokenizer_en.decode([token for token in generated_tokens if token < tokenizer_en.vocab_size])\n",
        "\n",
        "\n",
        "    return translated_sentence\n",
        "\n",
        "\n",
        "for i, (pt_tensor, en_tensor) in enumerate(test_examples.take(num_test_examples)):\n",
        "    try:\n",
        "        # Get the translated sentence\n",
        "        translated_sentence = translate_sentence(pt_tensor, transformer, tokenizer_pt, tokenizer_en, MAX_LENGTH, d_model)\n",
        "\n",
        "        en_string = en_tensor.numpy().decode('utf-8')\n",
        "\n",
        "        reference_words = en_string.split()\n",
        "        hypothesis_words = translated_sentence.split()\n",
        "\n",
        "        references.append([reference_words]) # List of lists for references\n",
        "        hypotheses.append(hypothesis_words) # List of words for hypothesis\n",
        "\n",
        "        if (i + 1) % 20 == 0:\n",
        "            print(f\"Processed {i + 1}/{num_test_examples} examples.\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing example {i + 1}: {e}\")\n",
        "        hypotheses.append([])\n",
        "\n",
        "        try:\n",
        "             pt_string_for_ref = pt_tensor.numpy().decode('utf-8')\n",
        "             references.append([pt_string_for_ref.split()])\n",
        "        except:\n",
        "             references.append([[\"error\"]])\n",
        "\n",
        "\n",
        "# Calculate BLEU score\n",
        "\n",
        "try:\n",
        "    bleu_score = nltk.translate.bleu_score.corpus_bleu(\n",
        "        references,\n",
        "        hypotheses,\n",
        "        smoothing_function=nltk.translate.bleu_score.SmoothingFunction().method1\n",
        "    )\n",
        "    print(f\"\\nCorpus BLEU Score on {len(hypotheses)} examples: {bleu_score:.4f}\")\n",
        "except ZeroDivisionError:\n",
        "    print(\"\\nCould not compute BLEU score. This might happen if the generated hypotheses are empty.\")\n",
        "    bleu_score = 0.0\n",
        "    print(f\"Corpus BLEU Score: {bleu_score:.4f}\")\n",
        "\n",
        "end_time = time.time()\n",
        "print(f\"BLEU evaluation completed in {end_time - start_time:.2f} seconds.\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Evaluating BLEU score on 100 examples...\n",
            "Processed 20/100 examples.\n",
            "Processed 40/100 examples.\n",
            "Processed 60/100 examples.\n",
            "Processed 80/100 examples.\n",
            "Processed 100/100 examples.\n",
            "\n",
            "Corpus BLEU Score on 100 examples: 0.1963\n",
            "BLEU evaluation completed in 107.52 seconds.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "BLEU Score Analysis: BLEU Score: 0.1963\n",
        "\n",
        "About BLEU Score:\n",
        "The model's generated translations have some overlap with the reference translations in terms of n-grams, but the overall quality is relatively low.\n",
        "\n",
        "BLEU scores typically range from 0 to 1, where higher scores indicate better translation quality. So bad score.\n",
        "\n",
        "Model Performance Discussion:\n",
        "- This score suggests that the simplified Transformer model, trained on a subset of dataset for Portuguese to English translation, is possibly capable of generating some meaningful phrases and word sequences that align with the references, but it likely struggles with capturing the full meaning, fluency, and grammatical correctness of the target sentences.\n",
        "- The relatively low score is because of the simplified architecture (2 encoder/decoder layers, scaled dot-product attention only) compared to full Transformer models, the limited training data size, and potentially limited training epochs (5 epochs).\n"
      ],
      "metadata": {
        "id": "4EiwPFxEowls"
      }
    }
  ]
}